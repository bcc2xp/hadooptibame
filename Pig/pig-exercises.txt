//Retrieve temperature data from GitHub
wget https://github.com/ywchiu/hadoopiii2/raw/master/Pig/2014.zip

//Unzip data file
unzip 2014.zip

//Create directory, data
hadoop fs -mkdir data

//Upload 2014 data folder
hadoop fs -put 2014 /user/cloudera 

// Use pig:
pig

//Load temperature data from HDFS
allDataWithHeaders = LOAD '/user/cloudera/2014/*.op' AS (row:chararray);

//Transform Data
dataOnly = FILTER allDataWithHeaders BY SIZE(row) > 108L;
selectColumns = FOREACH dataOnly GENERATE
	      (int)TRIM(SUBSTRING(row, 0, 6)) AS station,
	      (int)TRIM(SUBSTRING(row, 7,12)) AS wban,
	      (int)TRIM(SUBSTRING(row, 14, 22)) AS ymd,
	      (float)TRIM(SUBSTRING(row, 24, 29)) AS temp_avg,
	      (float)TRIM(SUBSTRING(row, 102, 107)) AS temp_max,
	      (float)TRIM(SUBSTRING(row, 110, 115)) AS temp_min,
	      (float)TRIM(SUBSTRING(row, 78, 82)) AS windspeed_avg,
	      (float)TRIM(SUBSTRING(row, 95, 99)) AS windspeed_max;

//Clean Data
hasAllData = FILTER selectColumns by temp_avg < 900.0 AND temp_max < 900.0 AND temp_min < 900.0 AND windspeed_avg < 900.0 AND windspeed_max < 900.0;
STORE hasAllData INTO 'data/gsod-filtered';

//Sampling 10% data from dataset
sampleOnePercent = SAMPLE selectColumns 0.01;
STORE sampleOnePercent INTO 'data/gsod-one-pct';

//Order data by temperature
allData = LOAD 'data/gsod-filtered' USING PigStorage('\t') AS (station:int,wban:int,ymd:int,temp_avg:float,temp_max:float,temp_min:float,windspeed_avg:float,windspeed_max:float);
hottest = ORDER allData BY $4 DESC;
maxTemps = LIMIT hottest 10;
dump maxTemps;

//Get ISH-HISTORY.tsv from GitHub
wget https://github.com/ywchiu/hadoopiii2/raw/master/Pig/ISH-HISTORY.tsv

//Upload ISH-HISTORY.tsv to HDFS
hadoop fs -put ISH-HISTORY.tsv /user/cloudera/

//Merge station name with join 
stations = LOAD 'ISH-HISTORY.tsv' USING PigStorage('\t') AS (station:int,wban:int,station_name:chararray,country:chararray,state:chararray);
joined = JOIN maxTemps BY (station,wban), stations BY (station,wban);
ordered = ORDER joined BY $4 DESC;
DUMP ordered;
